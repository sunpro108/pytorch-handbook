{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ddd9a9-6f51-4a62-bf33-16ea2850fb65",
   "metadata": {},
   "source": [
    "## 3.4 CLIP+DINOv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d76a9-d700-4eb1-b2d9-f840b5939dc9",
   "metadata": {},
   "source": [
    "### CLIP-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a94446-3fcc-4662-bc61-edb81f1f9ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPModel\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db1bd61-2853-44ad-bf0a-7d8f7272286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "processor = AutoProcessor.from_pretrained(\"/sun/home_models/clip-vit-large-patch14\")\n",
    "model = CLIPModel.from_pretrained(\"/sun/home_models/clip-vit-large-patch14\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6014666-26fc-496a-86e7-7b61f3b0d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from image1\n",
    "image1 = Image.open('/sun/home_datasets/iharmony4_t_256/'\n",
    "                    'Hday2night/composite_images/d16761-82_1_1.png')\n",
    "with torch.no_grad():\n",
    "    inputs1 = processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "    image_features1 = model.get_image_features(**inputs1)\n",
    "\n",
    "#Extract features from image2\n",
    "image2 = Image.open('/sun/home_datasets/iharmony4_t_256/'\n",
    "                    'Hday2night/real_images/d16761-82.png')\n",
    "with torch.no_grad():\n",
    "    inputs2 = processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "    image_features2 = model.get_image_features(**inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "911d466b-24bd-44c2-b5f5-82509a908b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9802383184432983\n"
     ]
    }
   ],
   "source": [
    "#Compute their cosine similarity and convert it into a score between 0 and 1\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "sim = cos(image_features1[0],image_features2[0]).item()\n",
    "sim = (sim+1)/2\n",
    "print('Similarity:', sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b402332-09cf-4d2f-af89-a76bebf7c628",
   "metadata": {},
   "source": [
    "### 真图的前后景的相关性和合成图的前后景的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdeeea6-b5f5-4c70-bdcc-8a452186492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from image1\n",
    "image1 = Image.open('/sun/home_datasets/iharmony4_t_256/'\n",
    "                    'Hday2night/composite_images/d16761-82_1_1.png')\n",
    "with torch.no_grad():\n",
    "    inputs1 = processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "    image_features1 = model.get_image_features(**inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3e2130-11e5-4023-8134-8f73745923ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b732883-c5d4-4451-a306-7dc642b53f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from image2\n",
    "image2 = Image.open('/sun/home_datasets/iharmony4_t_256/'\n",
    "                    'Hday2night/real_images/d16761-82.png')\n",
    "with torch.no_grad():\n",
    "    inputs2 = processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "    image_features2 = model.get_image_features(**inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1ac094-fcb4-45c2-8ee1-69544732daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9802383184432983\n"
     ]
    }
   ],
   "source": [
    "#Compute their cosine similarity and convert it into a score between 0 and 1\n",
    "cos = nn.CosineSimilarity(dim=0)\n",
    "sim = cos(image_features1[0],image_features2[0]).item()\n",
    "sim = (sim+1)/2\n",
    "print('Similarity:', sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe82695-b0e5-4179-b0d0-f22a9f826d63",
   "metadata": {},
   "source": [
    "### DINOV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c73132-2de4-4b46-a3ab-0c083a51663a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Load DINOv2 model and processor\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m processor_dino \u001b[38;5;241m=\u001b[39m \u001b[43mAutoImageProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/sun/home_models/dinov2/dinov2_vitl14_pretrain.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model_dino \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/sun/home_models/dinov2/dinov2_vitl14_pretrain.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py:320\u001b[0m, in \u001b[0;36mAutoImageProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    318\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43mImageProcessingMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m image_processor_class \u001b[38;5;241m=\u001b[39m config_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_processor_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    322\u001b[0m image_processor_auto_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py:328\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m--> 328\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m     image_processor_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(text)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPModel, AutoImageProcessor, AutoModel\n",
    "# import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Load DINOv2 model and processor\n",
    "processor_dino = AutoImageProcessor.from_pretrained('/sun/home_models/dinov2/dinov2_vitl14_pretrain.pth')\n",
    "model_dino = AutoModel.from_pretrained('/sun/home_models/dinov2/dinov2_vitl14_pretrain.pth').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175ea437-e667-48ba-8428-e698cecd4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from image1\n",
    "image1 = Image.open('/sun/home_datasets/iharmony4_t_256/'\n",
    "                    'Hday2night/composite_images/d16761-82_1_1.png')\n",
    "with torch.no_grad():\n",
    "    inputs1 = processor_dino(images=image1, return_tensors=\"pt\").to(device)\n",
    "    image_features1 = model_dino(**inputs1)\n",
    "\n",
    "#Extract features from image2\n",
    "image2 = Image.open('/sun/home_datasets/iharmony4_t_256/'\n",
    "                    'Hday2night/real_images/d16761-82.png')\n",
    "with torch.no_grad():\n",
    "    inputs2 = processor_dino(images=image2, return_tensors=\"pt\").to(device)\n",
    "    image_features2 = model_dino(**inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07bb125-3589-414e-8d97-4ed0403fde12",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Retrieve all filenames\n",
    " images = []\n",
    " for root, dirs, files in os.walk('./val2017/'):\n",
    "    for file in files:\n",
    "        if file.endswith('jpg'):\n",
    "            images.append(root + '/'+ file)\n",
    "\n",
    "\n",
    " #Define a function that normalizes embeddings and add them to the index\n",
    " def add_vector_to_index(embedding, index):\n",
    "    #convert embedding to numpy\n",
    "    vector = embedding.detach().cpu().numpy()\n",
    "    #Convert to float32 numpy\n",
    "    vector = np.float32(vector)\n",
    "    #Normalize vector: important to avoid wrong results when searching\n",
    "    faiss.normalize_L2(vector)\n",
    "    #Add to index\n",
    "    index.add(vector)\n",
    "\n",
    " def extract_features_clip(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_clip(images=image, return_tensors=\"pt\").to(device)\n",
    "        image_features = model_clip.get_image_features(**inputs)\n",
    "        return image_features\n",
    "\n",
    " def extract_features_dino(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_dino(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model_dino(**inputs)\n",
    "        image_features = outputs.last_hidden_state\n",
    "        return image_features.mean(dim=1)\n",
    "\n",
    " #Create 2 indexes.\n",
    " index_clip = faiss.IndexFlatL2(512)\n",
    " index_dino = faiss.IndexFlatL2(768)\n",
    "\n",
    " #Iterate over the dataset to extract features X2 and store features in indexes\n",
    " for image_path in images:\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    clip_features = extract_features_clip(img)\n",
    "    add_vector_to_index(clip_features,index_clip)\n",
    "    dino_features = extract_features_dino(img)\n",
    "    add_vector_to_index(dino_features,index_dino)\n",
    "\n",
    " #store the indexes locally\n",
    " faiss.write_index(index_clip,\"clip.index\")\n",
    " faiss.write_index(index_dino,\"dino.index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
